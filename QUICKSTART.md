# Guide de DÃ©marrage Rapide - Windows PowerShell

## ğŸš€ DÃ©marrage en 5 Minutes

### PrÃ©requis VÃ©rification
```powershell
# VÃ©rifier Docker
docker --version
docker compose version

# VÃ©rifier les fichiers CSV
dir data\*.csv
```

---

## ğŸ“‹ Commandes Essentielles (Copier-Coller)

### 1. DÃ©marrer l'Infrastructure (2-5 min)
```powershell
# Dans le rÃ©pertoire du projet
cd C:\Users\mayssen\bigdata-project

# Lancer tous les services
docker compose up -d

# Attendre que tout soit prÃªt (2-5 minutes)
Start-Sleep -Seconds 120

# VÃ©rifier l'Ã©tat
docker compose ps
```

### 2. Initialisation ComplÃ¨te (5-10 min)
```powershell
# Installer les dÃ©pendances Python
docker exec -it python-env python /scripts/install_dependencies.py

# Initialiser HDFS (charger les donnÃ©es)
docker exec -it hadoop-master bash -c "chmod +x /scripts/init_hdfs.sh && /scripts/init_hdfs.sh"

# Initialiser Kafka (crÃ©er le topic)
docker exec -it kafka bash -c "chmod +x /scripts/init_kafka.sh && /scripts/init_kafka.sh"

# Initialiser Cassandra (crÃ©er les tables)
docker exec -it cassandra bash -c "chmod +x /scripts/init_cassandra.sh && /scripts/init_cassandra.sh"

# Initialiser Hive (crÃ©er la base)
docker exec -it hive bash -c "chmod +x /scripts/init_hive.sh && /scripts/init_hive.sh"
```

### 3. Lancer le Batch Layer (Traitement Historique)
```powershell
# Soumettre le job Spark batch
docker exec -it spark-master spark-submit --master spark://spark-master:7077 --deploy-mode client /scripts/batch_job.py
```

### 4. Lancer le Speed Layer (Temps RÃ©el)

**Terminal 1 - Spark Streaming:**
```powershell
# Mode console (pour voir les rÃ©sultats)
docker exec -it spark-master spark-submit --master spark://spark-master:7077 --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,com.datastax.spark:spark-cassandra-connector_2.12:3.3.0 /scripts/streaming_job.py console

# OU Mode Cassandra (production)
docker exec -it spark-master spark-submit --master spark://spark-master:7077 --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,com.datastax.spark:spark-cassandra-connector_2.12:3.3.0 /scripts/streaming_job.py cassandra
```

**Terminal 2 - Producteur Kafka:**
```powershell
# Ouvrir un nouveau terminal PowerShell
cd C:\Users\mayssen\bigdata-project

# Lancer le producteur
docker exec -it python-env python /scripts/producer_flights.py
```

### 5. VÃ©rifier les RÃ©sultats

**Hive (Batch):**
```powershell
# Se connecter Ã  Hive
docker exec -it hive beeline -u jdbc:hive2://localhost:10000

# Dans Beeline, exÃ©cuter:
# USE batch_views;
# SELECT * FROM airport_delay_stats ORDER BY avg_delay DESC LIMIT 10;
# !quit
```

**Cassandra (Temps RÃ©el):**
```powershell
# Se connecter Ã  Cassandra
docker exec -it cassandra cqlsh

# Dans CQL, exÃ©cuter:
# USE realtime;
# SELECT * FROM recent_delays LIMIT 10;
# exit
```

**Kafka (Messages):**
```powershell
# Voir les messages Kafka
docker exec -it kafka kafka-console-consumer.sh --topic live-flights --from-beginning --bootstrap-server localhost:9092 --max-messages 5
```

---

## ğŸŒ Interfaces Web

```powershell
# Ouvrir les interfaces web
start http://localhost:9870  # HDFS
start http://localhost:8080  # Spark Master
```

---

## ğŸ›‘ ArrÃªter Proprement

```powershell
# ArrÃªter tous les services
docker compose down

# ArrÃªter et supprimer les volumes (ATTENTION: perte de donnÃ©es)
docker compose down -v
```

---

## ğŸ” Monitoring en Temps RÃ©el

```powershell
# Voir les logs en temps rÃ©el
docker compose logs -f

# Logs d'un service spÃ©cifique
docker compose logs -f spark-master

# Statistiques des conteneurs
docker stats
```

---

## âš¡ Commandes Rapides

```powershell
# RedÃ©marrer un service
docker compose restart [service_name]

# Entrer dans un conteneur
docker exec -it [container_name] bash

# Voir l'utilisation des ressources
docker stats --no-stream

# Nettoyer Docker
docker system prune -a
```

---

## ğŸ› DÃ©pannage Express

**ProblÃ¨me: Conteneur ne dÃ©marre pas**
```powershell
docker compose logs [service_name]
docker compose restart [service_name]
```

**ProblÃ¨me: Port occupÃ©**
```powershell
netstat -ano | findstr :[PORT]
# ArrÃªter le processus ou modifier docker-compose.yml
```

**ProblÃ¨me: MÃ©moire insuffisante**
```
Docker Desktop â†’ Settings â†’ Resources â†’ Augmenter RAM Ã  8+ GB
```

**RedÃ©marrage complet:**
```powershell
docker compose down
docker compose up -d
Start-Sleep -Seconds 120
docker compose ps
```

---

## ğŸ“Š Workflow Complet (Script Automatique)

CrÃ©ez un fichier `run_full_pipeline.ps1`:

```powershell
# Script de lancement complet
Write-Host "ğŸš€ DÃ©marrage du pipeline Big Data..." -ForegroundColor Green

# 1. DÃ©marrer les services
Write-Host "ğŸ“¦ Lancement des conteneurs..." -ForegroundColor Yellow
docker compose up -d
Start-Sleep -Seconds 120

# 2. Initialisation
Write-Host "âš™ï¸ Initialisation..." -ForegroundColor Yellow
docker exec -it python-env python /scripts/install_dependencies.py
docker exec -it hadoop-master bash -c "/scripts/init_hdfs.sh"
docker exec -it kafka bash -c "/scripts/init_kafka.sh"
docker exec -it cassandra bash -c "/scripts/init_cassandra.sh"

# 3. Batch Layer
Write-Host "ğŸ“Š Lancement du Batch Layer..." -ForegroundColor Yellow
docker exec -it spark-master spark-submit --master spark://spark-master:7077 /scripts/batch_job.py

Write-Host "âœ… Pipeline prÃªt! Lancez manuellement le Speed Layer." -ForegroundColor Green
Write-Host "Terminal 1: docker exec -it spark-master spark-submit ... /scripts/streaming_job.py console" -ForegroundColor Cyan
Write-Host "Terminal 2: docker exec -it python-env python /scripts/producer_flights.py" -ForegroundColor Cyan
```

ExÃ©cuter:
```powershell
.\run_full_pipeline.ps1
```

---

**ğŸ‰ Vous Ãªtes prÃªt! Happy Big Data Processing! ğŸš€**
