"""
Kafka Producer pour ingÃ©rer les donnÃ©es de vols depuis CSV vers Kafka
"""
from kafka import KafkaProducer
import pandas as pd
import json
import time
import sys
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def create_producer(bootstrap_servers='kafka:9092', max_retries=10):
    """CrÃ©e un producer Kafka avec retry logic"""
    for attempt in range(max_retries):
        try:
            producer = KafkaProducer(
                bootstrap_servers=bootstrap_servers,
                value_serializer=lambda v: json.dumps(v).encode('utf-8'),
                max_request_size=10485760,  # 10MB
                buffer_memory=33554432,  # 32MB
                compression_type='gzip'
            )
            logger.info("âœ“ Connexion Ã  Kafka rÃ©ussie")
            return producer
        except Exception as e:
            logger.warning(f"Tentative {attempt + 1}/{max_retries} - Erreur: {e}")
            time.sleep(5)
    
    raise Exception("Impossible de se connecter Ã  Kafka")

def send_flights_to_kafka(csv_file, topic='live-flights', batch_size=100, delay=0.01):
    """
    Envoie les donnÃ©es de vols vers Kafka
    
    Args:
        csv_file: Chemin vers le fichier CSV
        topic: Nom du topic Kafka
        batch_size: Nombre de messages Ã  envoyer avant flush
        delay: DÃ©lai entre les messages (en secondes)
    """
    try:
        # Connexion au producer
        producer = create_producer()
        
        # Lecture du CSV par chunks pour Ã©conomiser la mÃ©moire
        logger.info(f"ğŸ“– Lecture du fichier {csv_file}...")
        chunk_size = 10000
        total_sent = 0
        
        for chunk_idx, df_chunk in enumerate(pd.read_csv(csv_file, chunksize=chunk_size)):
            logger.info(f"ğŸ“¦ Traitement du chunk {chunk_idx + 1} ({len(df_chunk)} lignes)...")
            
            for idx, row in df_chunk.iterrows():
                try:
                    # Conversion en dictionnaire et nettoyage des NaN
                    message = row.to_dict()
                    # Remplacer NaN par None pour JSON
                    message = {k: (None if pd.isna(v) else v) for k, v in message.items()}
                    
                    # Envoi vers Kafka
                    producer.send(topic, value=message)
                    total_sent += 1
                    
                    # Flush pÃ©riodique
                    if total_sent % batch_size == 0:
                        producer.flush()
                        logger.info(f"âœ“ {total_sent} messages envoyÃ©s")
                    
                    # DÃ©lai pour Ã©viter de surcharger
                    if delay > 0:
                        time.sleep(delay)
                        
                except Exception as e:
                    logger.error(f"Erreur lors de l'envoi du message {idx}: {e}")
                    continue
        
        # Flush final
        producer.flush()
        logger.info(f"âœ“ TerminÃ©! Total de {total_sent} messages envoyÃ©s au topic '{topic}'")
        producer.close()
        
    except FileNotFoundError:
        logger.error(f"âŒ Fichier non trouvÃ©: {csv_file}")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ Erreur: {e}")
        sys.exit(1)

if __name__ == "__main__":
    # Configuration
    CSV_FILE = "/data/2018.csv"  # Chemin dans le conteneur Docker
    TOPIC = "live-flights"
    
    # ParamÃ¨tres personnalisables via arguments
    if len(sys.argv) > 1:
        CSV_FILE = sys.argv[1]
    if len(sys.argv) > 2:
        TOPIC = sys.argv[2]
    
    logger.info("=" * 60)
    logger.info("ğŸš€ DÃ©marrage du producteur Kafka pour donnÃ©es de vols")
    logger.info("=" * 60)
    logger.info(f"ğŸ“ Fichier: {CSV_FILE}")
    logger.info(f"ğŸ“¡ Topic: {TOPIC}")
    logger.info("=" * 60)
    
    send_flights_to_kafka(CSV_FILE, TOPIC)
