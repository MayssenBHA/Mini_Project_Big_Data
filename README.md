# ğŸš€ Projet Big Data - PrÃ©diction des Retards de Vols
## Architecture Lambda avec Docker

Ce projet implÃ©mente une **Architecture Lambda** complÃ¨te pour l'analyse et la prÃ©diction des retards de vols en utilisant les technologies Big Data modernes, le tout orchestrÃ© avec Docker.

---

## ğŸ“‹ Table des MatiÃ¨res

1. [Vue d'ensemble](#-vue-densemble)
2. [Architecture](#-architecture)
3. [Technologies](#-technologies)
4. [PrÃ©requis](#-prÃ©requis)
5. [Installation](#-installation)
6. [Configuration](#-configuration)
7. [Utilisation](#-utilisation)
8. [Structure du Projet](#-structure-du-projet)
9. [Commandes Utiles](#-commandes-utiles)
10. [DÃ©pannage](#-dÃ©pannage)

---

## ğŸ¯ Vue d'ensemble

Ce projet analyse les donnÃ©es de vols des compagnies aÃ©riennes amÃ©ricaines pour prÃ©dire les retards. Il utilise l'architecture Lambda qui combine:
- **Batch Layer**: Traitement historique massif avec Spark et stockage dans Hive
- **Speed Layer**: Traitement temps rÃ©el avec Kafka et Spark Streaming vers Cassandra
- **Serving Layer**: Combinaison des vues batch et temps rÃ©el pour requÃªtes optimales

### Dataset
- **Source**: [Kaggle - Airline Delay and Cancellation Data (2009-2018)](https://www.kaggle.com/datasets/yuanyuwendymu/airline-delay-and-cancellation-data-2009-2018)
- **Taille**: ~5-10 GB (tous les fichiers CSV)
- **Lignes**: ~7 millions par annÃ©e
- **Colonnes**: 28 attributs (retards, aÃ©roports, distances, etc.)

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Architecture Lambda                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Dataset    â”‚ (CSV Files)
â”‚  Flights     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                                     â”‚
       â–¼                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   BATCH LAYER    â”‚              â”‚   SPEED LAYER    â”‚
â”‚                  â”‚              â”‚                  â”‚
â”‚  HDFS Storage    â”‚              â”‚  Kafka Ingestion â”‚
â”‚       â†“          â”‚              â”‚       â†“          â”‚
â”‚  Spark Batch     â”‚              â”‚ Spark Streaming  â”‚
â”‚       â†“          â”‚              â”‚       â†“          â”‚
â”‚  Hive Tables     â”‚              â”‚    Cassandra     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                 â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  SERVING LAYER   â”‚
              â”‚                  â”‚
              â”‚  Query Engine    â”‚
              â”‚  (Hive + CQL)    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Technologies

| Composant | Technologie | Version | Port |
|-----------|-------------|---------|------|
| **Stockage DistribuÃ©** | Apache Hadoop (HDFS) | 3.1.1 | 9870, 9000 |
| **Traitement Batch** | Apache Spark | 3.3.0 | 8080, 7077 |
| **Message Broker** | Apache Kafka | 7.0.1 | 9092, 29092 |
| **Coordination** | Apache Zookeeper | 7.0.1 | 2181 |
| **NoSQL Temps RÃ©el** | Apache Cassandra | 4.0 | 9042 |
| **Data Warehouse** | Apache Hive | 2.3.2 | 10000 |
| **Scripting** | Python | 3.9 | - |
| **Orchestration** | Docker Compose | 3 | - |

---

## ğŸ’» PrÃ©requis

### SystÃ¨me HÃ´te
- **OS**: Windows 10/11, macOS, ou Linux (Ubuntu 20.04+)
- **RAM**: **Minimum 16 GB** (recommandÃ© 32 GB)
- **Disque**: **Minimum 50 GB** d'espace libre
- **CPU**: 4 cÅ“urs minimum (recommandÃ© 8 cÅ“urs)

### Logiciels
- **Docker Desktop** (derniÃ¨re version)
  - Windows/Mac: [TÃ©lÃ©charger ici](https://www.docker.com/products/docker-desktop)
  - Linux: Docker Engine + Docker Compose
- **Git** (pour cloner le projet)

### VÃ©rification
```powershell
# VÃ©rifier Docker
docker --version
docker compose version

# VÃ©rifier les ressources Docker Desktop
# Ouvrir Docker Desktop â†’ Settings â†’ Resources
# Allouer au moins 8 GB RAM et 4 CPU cores
```

---

## ğŸ“¥ Installation

### Ã‰tape 1: Cloner ou CrÃ©er le Projet
```powershell
# Si le projet existe dÃ©jÃ 
cd C:\Users\mayssen\bigdata-project

# Sinon, crÃ©er la structure
mkdir bigdata-project
cd bigdata-project
mkdir data, scripts, configs, configs\hive
```

### Ã‰tape 2: TÃ©lÃ©charger le Dataset
1. Allez sur [Kaggle](https://www.kaggle.com/datasets/yuanyuwendymu/airline-delay-and-cancellation-data-2009-2018)
2. TÃ©lÃ©chargez le ZIP (~5 GB)
3. Extrayez les fichiers CSV dans `data/`
4. Pour tests, utilisez `2018.csv` (~7M lignes)

```powershell
# VÃ©rifier que les donnÃ©es sont prÃ©sentes
dir data\*.csv
```

### Ã‰tape 3: Lancer l'Infrastructure
```powershell
# Lancer tous les conteneurs
docker compose up -d

# VÃ©rifier que tous les services sont dÃ©marrÃ©s (peut prendre 2-5 minutes)
docker compose ps

# Vous devriez voir 8 conteneurs: hadoop-master, hadoop-datanode, 
# spark-master, spark-worker, kafka, zookeeper, cassandra, hive, python-env
```

### Ã‰tape 4: Attendre l'Initialisation
```powershell
# Attendre que tous les services soient prÃªts (2-5 minutes)
# VÃ©rifier les logs
docker compose logs -f
# Appuyer sur Ctrl+C pour sortir

# VÃ©rifier les interfaces web
# HDFS: http://localhost:9870
# Spark: http://localhost:8080
```

---

## âš™ï¸ Configuration

### 1. Initialiser HDFS
```powershell
# Charger les donnÃ©es dans HDFS
docker exec -it hadoop-master bash
/scripts/init_hdfs.sh
exit
```

### 2. Initialiser Kafka
```powershell
# CrÃ©er le topic Kafka
docker exec -it kafka bash
/scripts/init_kafka.sh
exit
```

### 3. Initialiser Cassandra
```powershell
# CrÃ©er le keyspace et la table
docker exec -it cassandra bash
/scripts/init_cassandra.sh
exit
```

### 4. Initialiser Hive
```powershell
# CrÃ©er la base de donnÃ©es
docker exec -it hive bash
/scripts/init_hive.sh
exit
```

### 5. Installer les DÃ©pendances Python
```powershell
# Installer kafka-python, pandas, etc.
docker exec -it python-env python /scripts/install_dependencies.py
```

---

## ğŸ® Utilisation

### Flux Complet End-to-End

#### 1ï¸âƒ£ Batch Layer (Traitement Historique)
```powershell
# Soumettre le job batch Spark
docker exec -it spark-master spark-submit `
  --master spark://spark-master:7077 `
  --deploy-mode client `
  /scripts/batch_job.py

# VÃ©rifier les rÃ©sultats dans Hive
docker exec -it hive beeline -u jdbc:hive2://localhost:10000
# Dans Beeline:
> SHOW DATABASES;
> USE batch_views;
> SHOW TABLES;
> SELECT * FROM airport_delay_stats ORDER BY avg_delay DESC LIMIT 10;
> !quit
```

#### 2ï¸âƒ£ Speed Layer (Streaming Temps RÃ©el)

**Terminal 1: Lancer le Streaming Job**
```powershell
# Lancer Spark Streaming (console mode pour debug)
docker exec -it spark-master spark-submit `
  --master spark://spark-master:7077 `
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,com.datastax.spark:spark-cassandra-connector_2.12:3.3.0 `
  /scripts/streaming_job.py console

# OU vers Cassandra (production)
docker exec -it spark-master spark-submit `
  --master spark://spark-master:7077 `
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,com.datastax.spark:spark-cassandra-connector_2.12:3.3.0 `
  /scripts/streaming_job.py cassandra
```

**Terminal 2: Lancer le Producteur Kafka**
```powershell
# IngÃ©rer les donnÃ©es vers Kafka
docker exec -it python-env python /scripts/producer_flights.py

# Avec un fichier spÃ©cifique
docker exec -it python-env python /scripts/producer_flights.py /data/2018.csv live-flights
```

**VÃ©rifier Kafka**
```powershell
# Consommer les messages (autre terminal)
docker exec -it kafka kafka-console-consumer.sh `
  --topic live-flights `
  --from-beginning `
  --bootstrap-server localhost:9092 `
  --max-messages 10
```

#### 3ï¸âƒ£ Serving Layer (RequÃªtes CombinÃ©es)

**Query Cassandra (Temps RÃ©el)**
```powershell
docker exec -it cassandra cqlsh
# Dans CQL:
> USE realtime;
> SELECT * FROM recent_delays WHERE origin = 'JFK';
> SELECT * FROM recent_delays ORDER BY recent_delay DESC LIMIT 10;
> exit
```

**Query Hive (Batch)**
```powershell
docker exec -it hive beeline -u jdbc:hive2://localhost:10000
# Dans Beeline:
> USE batch_views;
> SELECT origin, avg_delay, total_flights, delay_rate 
  FROM airport_delay_stats 
  WHERE origin = 'JFK';
> !quit
```

---

## ğŸ“ Structure du Projet

```
bigdata-project/
â”œâ”€â”€ docker-compose.yml          # Orchestration des services
â”œâ”€â”€ data/                       # DonnÃ©es CSV (Ã  tÃ©lÃ©charger)
â”‚   â”œâ”€â”€ 2018.csv               # DonnÃ©es de vols 2018
â”‚   â””â”€â”€ ...                    # Autres annÃ©es si besoin
â”œâ”€â”€ scripts/                   # Scripts Python et Bash
â”‚   â”œâ”€â”€ producer_flights.py    # Producteur Kafka
â”‚   â”œâ”€â”€ batch_job.py          # Job Spark batch
â”‚   â”œâ”€â”€ streaming_job.py      # Job Spark streaming
â”‚   â”œâ”€â”€ init_kafka.sh         # Init Kafka
â”‚   â”œâ”€â”€ init_cassandra.sh     # Init Cassandra
â”‚   â”œâ”€â”€ init_hdfs.sh          # Init HDFS
â”‚   â”œâ”€â”€ init_hive.sh          # Init Hive
â”‚   â””â”€â”€ install_dependencies.py # Install Python packages
â”œâ”€â”€ configs/                   # Configurations
â”‚   â””â”€â”€ hive/                 # Configs Hive (si besoin)
â””â”€â”€ README.md                 # Ce fichier
```

---

## ğŸ”§ Commandes Utiles

### Docker Compose
```powershell
# DÃ©marrer tous les services
docker compose up -d

# ArrÃªter tous les services
docker compose down

# Voir les logs
docker compose logs -f [service_name]

# RedÃ©marrer un service
docker compose restart [service_name]

# Voir l'Ã©tat des conteneurs
docker compose ps
```

### AccÃ¨s aux Conteneurs
```powershell
# Hadoop
docker exec -it hadoop-master bash

# Spark
docker exec -it spark-master bash

# Kafka
docker exec -it kafka bash

# Cassandra
docker exec -it cassandra cqlsh

# Hive
docker exec -it hive beeline -u jdbc:hive2://localhost:10000

# Python
docker exec -it python-env bash
```

### Monitoring
```powershell
# HDFS Web UI
start http://localhost:9870

# Spark Master UI
start http://localhost:8080

# Statistiques Docker
docker stats
```

### HDFS
```powershell
# Lister les fichiers
docker exec -it hadoop-master hdfs dfs -ls /data/flights_raw

# Voir le contenu d'un fichier
docker exec -it hadoop-master hdfs dfs -cat /data/flights_raw/2018.csv | head

# Espace utilisÃ©
docker exec -it hadoop-master hdfs dfs -du -h /data
```

### Kafka
```powershell
# Lister les topics
docker exec -it kafka kafka-topics.sh --list --bootstrap-server localhost:9092

# DÃ©tails d'un topic
docker exec -it kafka kafka-topics.sh --describe --topic live-flights --bootstrap-server localhost:9092

# Consommer des messages
docker exec -it kafka kafka-console-consumer.sh --topic live-flights --from-beginning --bootstrap-server localhost:9092 --max-messages 5
```

### Cassandra
```powershell
# Se connecter
docker exec -it cassandra cqlsh

# Commandes CQL
DESCRIBE KEYSPACES;
USE realtime;
DESCRIBE TABLES;
SELECT COUNT(*) FROM recent_delays;
```

---

## ğŸ› DÃ©pannage

### ProblÃ¨me: Les conteneurs ne dÃ©marrent pas
```powershell
# VÃ©rifier les logs
docker compose logs

# RedÃ©marrer proprement
docker compose down
docker compose up -d
```

### ProblÃ¨me: MÃ©moire insuffisante
```powershell
# Ouvrir Docker Desktop â†’ Settings â†’ Resources
# Augmenter la RAM Ã  8-12 GB minimum
# RedÃ©marrer Docker Desktop
```

### ProblÃ¨me: Port dÃ©jÃ  utilisÃ©
```powershell
# VÃ©rifier les ports
netstat -ano | findstr :9870
netstat -ano | findstr :8080

# ArrÃªter le processus qui utilise le port
# Ou modifier les ports dans docker-compose.yml
```

### ProblÃ¨me: HDFS n'est pas accessible
```powershell
# VÃ©rifier que Hadoop est dÃ©marrÃ©
docker compose ps hadoop-master

# RedÃ©marrer HDFS
docker compose restart hadoop-master hadoop-datanode

# Attendre 30 secondes et vÃ©rifier
start http://localhost:9870
```

### ProblÃ¨me: Spark ne peut pas se connecter Ã  HDFS
```powershell
# VÃ©rifier la configuration rÃ©seau
docker network ls
docker network inspect bigdata-project_bigdata

# RedÃ©marrer Spark
docker compose restart spark-master spark-worker
```

### ProblÃ¨me: Kafka timeout
```powershell
# VÃ©rifier que Zookeeper est dÃ©marrÃ©
docker compose ps zookeeper

# RedÃ©marrer Kafka
docker compose restart zookeeper kafka

# Attendre 15 secondes
```

### ProblÃ¨me: Cassandra ne rÃ©pond pas
```powershell
# Cassandra peut prendre 1-2 minutes au dÃ©marrage
docker compose logs cassandra

# Attendre le message "Starting listening for CQL clients"
# Puis tester:
docker exec -it cassandra cqlsh
```

### Nettoyage Complet
```powershell
# ArrÃªter et supprimer tous les conteneurs et volumes
docker compose down -v

# Supprimer les images (optionnel)
docker system prune -a

# RedÃ©marrer from scratch
docker compose up -d
```

---

## ğŸ“Š RÃ©sultats Attendus

### Batch Layer
- Table Hive `batch_views.airport_delay_stats` avec statistiques par aÃ©roport
- Colonnes: Origin, avg_delay, avg_dep_delay, total_flights, delayed_flights, delay_rate
- ~300+ aÃ©roports analysÃ©s

### Speed Layer
- Table Cassandra `realtime.recent_delays` mise Ã  jour en temps rÃ©el
- Colonnes: origin (PK), recent_delay, recent_dep_delay
- Latence < 5 secondes

### Serving Layer
- RequÃªtes combinÃ©es pour avoir vue complÃ¨te (historique + temps rÃ©el)
- API ou dashboard (Ã  implÃ©menter) pour visualisation

---

## ğŸ“ Prochaines Ã‰tapes

1. **Machine Learning**: Ajouter un modÃ¨le de prÃ©diction (Random Forest, Gradient Boosting)
2. **Visualisation**: Dashboard avec Grafana ou Superset
3. **API REST**: Exposer les donnÃ©es via FastAPI ou Flask
4. **Automatisation**: Orchestration avec Airflow
5. **Monitoring**: Prometheus + Grafana pour mÃ©triques systÃ¨me

---

## ğŸ¤ Contribution

Pour toute question ou amÃ©lioration, crÃ©ez une issue ou un pull request.

---

## ğŸ“„ Licence

Ce projet est Ã  des fins Ã©ducatives.

---

**Bon apprentissage! ğŸš€**
